{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQyROdrfsvk4"
      },
      "source": [
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/Majdoddin/nlp/blob/main/Pyannote_plays_and_Whisper_rhymes_v_2_0.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/majdoddin/nlp)\n",
        "\n",
        "# Whisper's transcription plus Pyannote's Diarization\n",
        "\n",
        "**Update** - [@johnwyles](https://github.com/johnwyles) added HTML output for audio/video files from Google Drive, along with some fixes.\n",
        "\n",
        "Using the new word-level timestamping of Whisper, the transcription words are highlighted as the video plays, with optional autoscroll. And the display on small displays is improved.\n",
        "\n",
        "Moreover, the model is loaded just once, thus the whole thing runs much faster now. You can also hardcode your Huggingface token.\n",
        "\n",
        "---\n",
        "Andrej Karpathy [suggested](https://twitter.com/karpathy/status/1574476200801538048?s=20&t=s5IMMXOYjBI6-91dib6w8g) training a classifier on top of  OpenAI [Whisper](https://openai.com/blog/whisper/) model features to identify the speaker, so we can visualize the speaker in the transcript. But, as [pointed out](https://twitter.com/tarantulae/status/1574493613362388992?s=20&t=s5IMMXOYjBI6-91dib6w8g) by Christian Perone, it seems that features from whisper wouldn't be that great for speaker recognition as its main objective is basically to ignore speaker differences.\n",
        "\n",
        "In the following, I use [**`pyannote-audio`**](https://github.com/pyannote/pyannote-audio), a speaker diarization toolkit by Hervé Bredin, to identify the speakers, and then match it with the transcriptions of Whispr, linked to the video. The input can be YouTube or an video/audio file (also on Google Drive). I try it on a [Customer Support Call](https://youtu.be/hpZFJctBUHQ). Check the result [**here**](https://majdoddin.github.io/dyson.html).\n",
        "\n",
        "To make it easier to match the transcriptions to diarizations by speaker change, Sarah Kaiser [suggested](https://github.com/openai/whisper/discussions/264#discussioncomment-3825375) runnnig the pyannote.audio first and  then just running whisper on the split-by-speaker chunks.\n",
        "For sake of performance (and transcription quality?), we attach the audio segements into a single audio file with a silent spacer as a seperator, and run whisper on it. Enjoy it!\n",
        "\n",
        "(For sake of performance , I also tried attaching the audio segements into a single audio file with a silent -or beep- spacer as a seperator, and run whisper on it see it on [colab](https://colab.research.google.com/drive/1HuvcY4tkTHPDzcwyVH77LCh_m8tP-Qet?usp=sharing). It [works](https://majdoddin.github.io/lexicap.html) on some audio, and fails on some (Dyson's Interview). The problem is, whisper does not reliably make a timestap on a spacer. See the discussions [#139](https://github.com/openai/whisper/discussions/139) and [#29](https://github.com/openai/whisper/discussions/29))\n",
        "\n",
        "The Markdown form used below is from [@ArthurFDLR](https://github.com/ArthurFDLR/whisper-youtube/).   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtljXaTXnowa"
      },
      "source": [
        "# Preparing the audio file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUA_4lnNgpSW"
      },
      "source": [
        "**Optional:** Mount Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1zqnZsBacKph",
        "vscode": {
          "languageId": "python"
        },
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "#@markdown Enter the URL of the YouTube video, or the path to the video/audio file you want to transcribe, give the output path, etc. and run the cell. HTML file embeds the video for YouTube, and audio for media files.\n",
        "\n",
        "Source = 'Youtube' #@param ['Youtube', 'File (Google Drive)']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video**\n",
        "video_url = \"https://www.youtube.com/watch?v=s5FwsycmN5Q\" #@param {type:\"string\"}\n",
        "#store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video or audio path (mp4, wav, mp3)**\n",
        "video_path = \"\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "output_path = \"/content/transscript/\" #@param {type:\"string\"}\n",
        "output_path = str(Path(output_path))\n",
        "#@markdown ---\n",
        "#@markdown #### **Title for transcription of media file**\n",
        "audio_title = \"googoosh_interview\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Copy a token from your [Hugging Face tokens page](https://huggingface.co/settings/tokens) and paste it below.\n",
        "access_token = \"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\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "NvDON2GxZpIb"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vgK82ahXNje",
        "outputId": "21845a2f-ecf0-4e2b-afb5-6e178ceb0083",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transscript\n"
          ]
        }
      ],
      "source": [
        "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
        "%cd {output_path}\n",
        "video_title = \"\"\n",
        "video_id = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-a6pLioVHjl"
      },
      "source": [
        "## From YouTube"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct4adIANpQvX"
      },
      "source": [
        " Installing [`yt-dlp`](https://github.com/yt-dlp/yt-dlp) and downloading the [video](https://youtu.be/NSp2fEQ6wyA) from youtube."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL7fC4aZdpyH",
        "outputId": "d32e0a7a-b77f-4c5f-e7fc-9b170e4d9882",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.7.6-py2.py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.5.7)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.0.9 mutagen-1.46.0 pycryptodomex-3.18.0 websockets-11.0.3 yt-dlp-2023.7.6\n"
          ]
        }
      ],
      "source": [
        "if Source == \"Youtube\":\n",
        "  !pip install -U yt-dlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI5sr2GI4gXb"
      },
      "source": [
        "Custom build of `ffmpeg` as [recommended](https://github.com/yt-dlp/yt-dlp#strongly-recommended) by `yt-dlp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KsedMPN-daEX",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "if Source == \"Youtube\":\n",
        "  !wget -O - -q  https://github.com/yt-dlp/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz | xz -qdc| tar -x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNcD7tx9UmyK",
        "outputId": "72417a9b-4930-49b6-8a57-861962941142",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=s5FwsycmN5Q\n",
            "[youtube] s5FwsycmN5Q: Downloading webpage\n",
            "[youtube] s5FwsycmN5Q: Downloading ios player API JSON\n",
            "[youtube] s5FwsycmN5Q: Downloading android player API JSON\n",
            "[youtube] s5FwsycmN5Q: Downloading m3u8 information\n",
            "Title: Googoosh Interview (2004): Part 1 of 8\n"
          ]
        }
      ],
      "source": [
        "#Getting video info\n",
        "if Source == \"Youtube\":\n",
        "  from yt_dlp import YoutubeDL\n",
        "  with YoutubeDL() as ydl:\n",
        "    info_dict = ydl.extract_info(video_url, download=False)\n",
        "    video_title = info_dict.get('title', None)\n",
        "    video_id = info_dict.get('id', None)\n",
        "    print(\"Title: \" + video_title) # <= Here, you got the video title\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_UyK49aPNiD"
      },
      "source": [
        "Downloading the audio from YouTube."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VHzpv72dkvV",
        "outputId": "9ffcaa44-9b32-4d12-9e66-1c054683edf6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[debug] Command-line config: ['-xv', '--ffmpeg-location', 'ffmpeg-master-latest-linux64-gpl/bin', '--audio-format', 'wav', '-o', '/content/transscript/input.wav', '--', 'https://www.youtube.com/watch?v=s5FwsycmN5Q']\n",
            "[debug] Encodings: locale UTF-8, fs utf-8, pref UTF-8, out utf-8, error utf-8, screen utf-8\n",
            "[debug] yt-dlp version stable@2023.07.06 [b532a3481] (pip)\n",
            "[debug] Python 3.10.12 (CPython x86_64 64bit) - Linux-5.15.109+-x86_64-with-glibc2.31 (OpenSSL 1.1.1f  31 Mar 2020, glibc 2.31)\n",
            "[debug] exe versions: ffmpeg N-111424-ge6954fd087-20230712 (setts), ffprobe N-111424-ge6954fd087-20230712\n",
            "[debug] Optional libraries: Cryptodome-3.18.0, brotli-1.0.9, certifi-2023.05.07, mutagen-1.46.0, sqlite3-2.6.0, websockets-11.0.3\n",
            "[debug] Proxy map: {'colab_language_server': '/usr/colab/bin/language_service'}\n",
            "[debug] Loaded 1855 extractors\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=s5FwsycmN5Q\n",
            "[youtube] s5FwsycmN5Q: Downloading webpage\n",
            "[youtube] s5FwsycmN5Q: Downloading ios player API JSON\n",
            "[youtube] s5FwsycmN5Q: Downloading android player API JSON\n",
            "[youtube] s5FwsycmN5Q: Downloading m3u8 information\n",
            "[debug] Sort order given by extractor: quality, res, fps, hdr:12, source, vcodec:vp9.2, channels, acodec, lang, proto\n",
            "[debug] Formats sorted by: hasvid, ie_pref, quality, res, fps, hdr:12(7), source, vcodec:vp9.2(10), channels, acodec, lang, proto, size, br, asr, vext, aext, hasaud, id\n",
            "[info] s5FwsycmN5Q: Downloading 1 format(s): 251\n",
            "[debug] Invoking http downloader on \"https://rr1---sn-5hnednss.googlevideo.com/videoplayback?expire=1689276416&ei=oPuvZP-KAoKI6dsPrvqP8AE&ip=34.90.100.71&id=o-AIE79vqYXTS3hz1F_Ur0gGTIIa5mHXQU46O9my42-Pvq&itag=251&source=youtube&requiressl=yes&mh=rC&mm=31%2C26&mn=sn-5hnednss%2Csn-5goeenez&ms=au%2Conr&mv=m&mvi=1&pl=21&spc=Ul2Sq4Pmb9kWeWqA7564zx8eXXhTXj0&vprv=1&svpuc=1&mime=audio%2Fwebm&gir=yes&clen=6635604&dur=537.681&lmt=1572971248780674&mt=1689254479&fvip=5&keepalive=yes&fexp=24007246%2C24362687&c=ANDROID&txp=1301222&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Cgir%2Cclen%2Cdur%2Clmt&sig=AOq0QJ8wRgIhAKEDjqOLyHlkKyrhkuaB_gPJj_oJolRtnOt3L6_x7FibAiEA0ov-hGS_6mLnhkdstYUKhNpwJnpRi15vasXx89QR3qI%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl&lsig=AG3C_xAwRQIgcvLuJjXq2gOLS82-RZbmG-WiPVagR5tvyc2AeNW1LR0CIQDV5rEGyPCB-ykrwZvv0cITXxX-hvZp9ZGMKeS-VS3nLA%3D%3D\"\n",
            "[download] Destination: /content/transscript/input.webm\n",
            "\u001b[K[download] 100% of    6.33MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m41.92MiB/s\u001b[0m\n",
            "[debug] ffmpeg command line: ffmpeg-master-latest-linux64-gpl/bin/ffprobe -show_streams file:/content/transscript/input.webm\n",
            "[ExtractAudio] Destination: /content/transscript/input.wav\n",
            "[debug] ffmpeg command line: ffmpeg-master-latest-linux64-gpl/bin/ffmpeg -y -loglevel repeat+info -i file:/content/transscript/input.webm -vn -movflags +faststart file:/content/transscript/input.wav\n",
            "Deleting original file /content/transscript/input.webm (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "if Source == \"Youtube\":\n",
        "  !yt-dlp -xv --ffmpeg-location ffmpeg-master-latest-linux64-gpl/bin --audio-format wav  -o \"{str(output_path) + '/'}input.wav\" -- {video_url}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJRyeo5RVYb8"
      },
      "source": [
        "## or from File (Google Drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VjORT6CkVoTF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "if Source == 'File (Google Drive)':\n",
        "    !ffmpeg -i {repr(video_path)} -vn -acodec pcm_s16le -ar 16000 -ac 1 -y input.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u1vbqd_VzNp"
      },
      "source": [
        "## Prepending a spacer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7qMLTISFE6M"
      },
      "source": [
        "`pyannote.audio` seems to miss the first 0.5 seconds of the audio, and, therefore, we prepend a spcacer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqznOE7Kw725",
        "outputId": "6850f43f-3c03-4eab-b8ce-d15ae1eacccb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaRDsBV1CWi8",
        "outputId": "a86803a9-2f01-4f62-f488-096c72eacb60",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='input_prep.wav'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "spacermilli = 2000\n",
        "spacer = AudioSegment.silent(duration=spacermilli)\n",
        "\n",
        "\n",
        "audio = AudioSegment.from_wav(\"input.wav\")\n",
        "\n",
        "audio = spacer.append(audio, crossfade=0)\n",
        "\n",
        "audio.export('input_prep.wav', format='wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb5eEOKUooju"
      },
      "source": [
        "# Pyannote's Diarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxNf1l8Ye_U9"
      },
      "source": [
        "[`pyannote.audio`](https://github.com/pyannote/pyannote-audio) is an open-source toolkit written in Python for **speaker diarization**.\n",
        "\n",
        "Based on [`PyTorch`](https://pytorch.org) machine learning framework, it provides a set of trainable end-to-end neural building blocks that can be combined and jointly optimized to build speaker diarization pipelines.\n",
        "\n",
        "`pyannote.audio` also comes with pretrained [models](https://huggingface.co/models?other=pyannote-audio-model) and [pipelines](https://huggingface.co/models?other=pyannote-audio-pipeline) covering a wide range of domains for voice activity detection, speaker segmentation, overlapped speech detection, speaker embedding reaching state-of-the-art performance for most of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Ak_OQwqd-3"
      },
      "source": [
        "Installing `pyannote.audio`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install light-the-torch"
      ],
      "metadata": {
        "id": "KFgcJ8f6dNUR",
        "outputId": "0ebddef1-5a29-4aa1-effe-6b7b42bec510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting light-the-torch\n",
            "  Downloading light_the_torch-0.7.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pip<23.2,>=22.3 in /usr/local/lib/python3.10/dist-packages (from light-the-torch) (23.1.2)\n",
            "Installing collected packages: light-the-torch\n",
            "Successfully installed light-the-torch-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ltt install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1"
      ],
      "metadata": {
        "id": "GhBEKgQGdPJc",
        "outputId": "489a1e17-1a0f-4b8d-e62b-1cd714baa0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJGyKTQJqdzq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install  git+https://github.com/hmmlearn/hmmlearn.git\n",
        "!pip install  git+https://github.com/pyannote/pyannote-audio.git@develop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7TPgEVW8XeH"
      },
      "source": [
        "**Important:** To load the pyannote speaker diarization pipeline,\n",
        "\n",
        "* accept the user conditions on both [hf.co/pyannote/speaker-diarization](https://hf.co/pyannote/speaker-diarization) and [hf.co/pyannote/segmentation](https://huggingface.co/pyannote/segmentation).\n",
        "* paste your access_token or login using `notebook_login` below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5u7VMb-YnqB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "if not(access_token):\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKG14DGYbwku",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization', use_auth_token= (access_token) or True )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipeline.to(device)"
      ],
      "metadata": {
        "id": "OjsUVdR2jH0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImKMcCr5W5Nw"
      },
      "source": [
        "Running pyannote.audio to generate the diarizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA4xiEefft9Z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "DEMO_FILE = {'uri': 'blabla', 'audio': 'input_prep.wav'}\n",
        "dz = pipeline(DEMO_FILE)\n",
        "\n",
        "with open(\"diarization.txt\", \"w\") as text_file:\n",
        "    text_file.write(str(dz))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHIY2MB3Vz3e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(*list(dz.itertracks(yield_label = True))[:10], sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp36eMedRkR0"
      },
      "source": [
        "# Preparing audio files according to the diarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPGOaVpOH7pZ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def millisec(timeStr):\n",
        "  spl = timeStr.split(\":\")\n",
        "  s = (int)((int(spl[0]) * 60 * 60 + int(spl[1]) * 60 + float(spl[2]) )* 1000)\n",
        "  return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Co3BIIH6aW4"
      },
      "source": [
        "Grouping the diarization segments according to the speaker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umQdzNFzcP2f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "dzs = open('diarization.txt').read().splitlines()\n",
        "\n",
        "groups = []\n",
        "g = []\n",
        "lastend = 0\n",
        "\n",
        "for d in dzs:\n",
        "  if g and (g[0].split()[-1] != d.split()[-1]):      #same speaker\n",
        "    groups.append(g)\n",
        "    g = []\n",
        "\n",
        "  g.append(d)\n",
        "\n",
        "  end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=d)[1]\n",
        "  end = millisec(end)\n",
        "  if (lastend > end):       #segment engulfed by a previous segment\n",
        "    groups.append(g)\n",
        "    g = []\n",
        "  else:\n",
        "    lastend = end\n",
        "if g:\n",
        "  groups.append(g)\n",
        "print(*groups, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOuf8CuRQeZo"
      },
      "source": [
        "Save the audio part corresponding to each diarization group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRQPUW4Mzvfn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "audio = AudioSegment.from_wav(\"input_prep.wav\")\n",
        "gidx = -1\n",
        "for g in groups:\n",
        "  start = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
        "  end = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[-1])[1]\n",
        "  start = millisec(start) #- spacermilli\n",
        "  end = millisec(end)  #- spacermilli\n",
        "  gidx += 1\n",
        "  audio[start:end].export(str(gidx) + '.wav', format='wav')\n",
        "  print(f\"group {gidx}: {start}--{end}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv2GYZCsLKBJ"
      },
      "source": [
        "Freeing up some memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cONumKWUjfus",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "del   DEMO_FILE, pipeline, spacer,  audio, dz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmxtB0k4n8lY"
      },
      "source": [
        "# Whisper's Transcriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swPVuqWaakkH"
      },
      "source": [
        "Installing Open AI whisper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUd7I__FUZVc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBO8IpdiRQ0X"
      },
      "source": [
        "Run whisper on all audio files. Whisper generates the transcription and writes it to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHKf0tFVmGyq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import whisper, torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = whisper.load_model('large', device = device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odstu62EnMLL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "for i in range(len(groups)):\n",
        "  audiof = str(i) + '.wav'\n",
        "  result = model.transcribe(audio=audiof, language='fa', word_timestamps=True)#, initial_prompt=result.get('text', \"\"))\n",
        "  with open(str(i)+'.json', \"w\") as outfile:\n",
        "    json.dump(result, outfile, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_UyWQMXpB3N"
      },
      "source": [
        "# Generating the HTML and/or txt file from the Transcriptions and the Diarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2qTkKD_30FG"
      },
      "source": [
        "Change or add to the speaker names and collors bellow as you wish `(speaker, textbox color, speaker color)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7EP6fO73wTY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "speakers = {'SPEAKER_00':('Aslan Hafezi', '#e1ffc7', 'darkgreen'), 'SPEAKER_01':('Googoosh', 'white', '#faafba'), 'SPEAKER_02':('Googoosh (singing)', 'white', '#faafba'), 'SPEAKER_03':('[audio description]', 'white', 'lightgrey') }\n",
        "def_boxclr = 'white'\n",
        "def_spkrclr = 'orange'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KndDYy_xMpMq"
      },
      "source": [
        "In the generated HTML,  the transcriptions for each diarization group are written in a box, with the speaker name on the top. By clicking a transcription, the embedded video jumps to the right time ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKdx9Hwg630K",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "if Source == 'Youtube':\n",
        "    preS = '<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n\\t<meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\\n\\t<title>' + \\\n",
        "video_title+ \\\n",
        "'</title>\\n\\t<style>\\n\\t\\tbody {\\n\\t\\t\\tfont-family: sans-serif;\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\tcolor: #111;\\n\\t\\t\\tpadding: 0 0 1em 0;\\n\\t\\t\\tbackground-color: #efe7dd;\\n\\t\\t}\\n\\n\\t\\ttable {\\n\\t\\t\\tborder-spacing: 10px;\\n\\t\\t}\\n\\n\\t\\tth {\\n\\t\\t\\ttext-align: left;\\n\\t\\t}\\n\\n\\t\\t.lt {\\n\\t\\t\\tcolor: inherit;\\n\\t\\t\\ttext-decoration: inherit;\\n\\t\\t}\\n\\n\\t\\t.l {\\n\\t\\t\\tcolor: #050;\\n\\t\\t}\\n\\n\\t\\t.s {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.c {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.e {\\n\\t\\t\\t/*background-color: white; Changing background color */\\n\\t\\t\\tborder-radius: 10px;\\n\\t\\t\\t/* Making border radius */\\n\\t\\t\\twidth: 50%;\\n\\t\\t\\t/* Making auto-sizable width */\\n\\t\\t\\tpadding: 0 0 0 0;\\n\\t\\t\\t/* Making space around letters */\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\t/* Changing font size */\\n\\t\\t\\tmargin-bottom: 0;\\n\\t\\t}\\n\\n\\t\\t.t {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t#player-div {\\n\\t\\t\\tposition: sticky;\\n\\t\\t\\ttop: 20px;\\n\\t\\t\\tfloat: right;\\n\\t\\t\\twidth: 40%\\n\\t\\t}\\n\\n\\t\\t#player {\\n\\t\\t\\taspect-ratio: 16 / 9;\\n\\t\\t\\twidth: 100%;\\n\\t\\t\\theight: auto;\\n\\n\\t\\t}\\n\\n\\t\\ta {\\n\\t\\t\\tdisplay: inline;\\n\\t\\t}\\n\\t</style>\\n\\t<script>\\n\\t\\tvar tag = document.createElement(\\'script\\');\\n\\t\\ttag.src = \"https://www.youtube.com/iframe_api\";\\n\\t\\tvar firstScriptTag = document.getElementsByTagName(\\'script\\')[0];\\n\\t\\tfirstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\\n\\t\\tvar player;\\n\\t\\tfunction onYouTubeIframeAPIReady() {\\n\\t\\t\\tplayer = new YT.Player(\\'player\\', {\\n\\t\\t\\t\\t//height: \\'210\\',\\n\\t\\t\\t\\t//width: \\'340\\',\\n\\t\\t\\t\\tvideoId: \\''+ \\\n",
        "video_id + \\\n",
        "'\\',\\n\\t\\t\\t});\\n\\n\\n\\n\\t\\t\\t// This is the source \"window\" that will emit the events.\\n\\t\\t\\tvar iframeWindow = player.getIframe().contentWindow;\\n\\t\\t\\tvar lastword = null;\\n\\n\\t\\t\\t// So we can compare against new updates.\\n\\t\\t\\tvar lastTimeUpdate = \"-1\";\\n\\n\\t\\t\\t// Listen to events triggered by postMessage,\\n\\t\\t\\t// this is how different windows in a browser\\n\\t\\t\\t// (such as a popup or iFrame) can communicate.\\n\\t\\t\\t// See: https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage\\n\\t\\t\\twindow.addEventListener(\"message\", function (event) {\\n\\t\\t\\t\\t// Check that the event was sent from the YouTube IFrame.\\n\\t\\t\\t\\tif (event.source === iframeWindow) {\\n\\t\\t\\t\\t\\tvar data = JSON.parse(event.data);\\n\\n\\t\\t\\t\\t\\t// The \"infoDelivery\" event is used by YT to transmit any\\n\\t\\t\\t\\t\\t// kind of information change in the player,\\n\\t\\t\\t\\t\\t// such as the current time or a playback quality change.\\n\\t\\t\\t\\t\\tif (\\n\\t\\t\\t\\t\\t\\tdata.event === \"infoDelivery\" &&\\n\\t\\t\\t\\t\\t\\tdata.info &&\\n\\t\\t\\t\\t\\t\\tdata.info.currentTime\\n\\t\\t\\t\\t\\t) {\\n\\t\\t\\t\\t\\t\\t// currentTime is emitted very frequently (milliseconds),\\n\\t\\t\\t\\t\\t\\t// but we only care about whole second changes.\\n\\t\\t\\t\\t\\t\\tvar ts = (data.info.currentTime).toFixed(1).toString();\\n\\t\\t\\t\\t\\t\\tts = (Math.round((data.info.currentTime) * 5) / 5).toFixed(1);\\n\\t\\t\\t\\t\\t\\tts = ts.toString();\\n\\t\\t\\t\\t\\t\\tconsole.log(ts)\\n\\t\\t\\t\\t\\t\\tif (ts !== lastTimeUpdate) {\\n\\t\\t\\t\\t\\t\\t\\tlastTimeUpdate = ts;\\n\\n\\t\\t\\t\\t\\t\\t\\t// It\\'s now up to you to format the time.\\n\\t\\t\\t\\t\\t\\t\\t//document.getElementById(\"time2\").innerHTML = time;\\n\\t\\t\\t\\t\\t\\t\\tword = document.getElementById(ts)\\n\\t\\t\\t\\t\\t\\t\\tif (word) {\\n\\t\\t\\t\\t\\t\\t\\t\\tif (lastword) {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlastword.style.fontWeight = \\'normal\\';\\n\\t\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\t\\tlastword = word;\\n\\t\\t\\t\\t\\t\\t\\t\\t//word.style.textDecoration = \\'underline\\';\\n\\t\\t\\t\\t\\t\\t\\t\\tword.style.fontWeight = \\'bold\\';\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tlet toggle = document.getElementById(\"autoscroll\");\\n\\t\\t\\t\\t\\t\\t\\t\\tif (toggle.checked) {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tlet position = word.offsetTop - 20;\\n\\t\\t\\t\\t\\t\\t\\t\\t\\twindow.scrollTo({\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttop: position,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tbehavior: \\'smooth\\'\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t\\t\\t}\\n\\n\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t})\\n\\t\\t}\\n\\t\\tfunction jumptoTime(timepoint, id) {\\n\\t\\t\\tevent.preventDefault();\\n\\t\\t\\thistory.pushState(null, null, \"#\" + id);\\n\\t\\t\\tplayer.seekTo(timepoint);\\n\\t\\t\\tplayer.playVideo();\\n\\t\\t}\\n\\t</script>\\n</head>\\n\\n<body>\\n\\t<h2>'  + \\\n",
        "video_title + \\\n",
        "'</h2>\\n\\t<i>Click on a part of the transcription, to jump to its video, and get an anchor to it in the address\\n\\t\\tbar<br><br></i>\\n\\t<div id=\"player-div\">\\n\\t\\t<div id=\"player\"></div>\\n\\t\\t<div><label for=\"autoscroll\">auto-scroll: </label>\\n\\t\\t\\t<input type=\"checkbox\" id=\"autoscroll\" checked>\\n\\t\\t</div>\\n\\t</div>\\n  '\n",
        "else:\n",
        "    preS = '\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n\\t<meta charset=\"UTF-8\">\\n\\t<meta name=\"viewport\" content=\"whtmlidth=device-width, initial-scale=1.0\">\\n\\t<meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\\n\\t<title>' + \\\n",
        "    audio_title+ \\\n",
        "    '</title>\\n\\t<style>\\n\\t\\tbody {\\n\\t\\t\\tfont-family: sans-serif;\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\tcolor: #111;\\n\\t\\t\\tpadding: 0 0 1em 0;\\n\\t\\t\\tbackground-color: #efe7dd;\\n\\t\\t}\\n\\n\\t\\ttable {\\n\\t\\t\\tborder-spacing: 10px;\\n\\t\\t}\\n\\n\\t\\tth {\\n\\t\\t\\ttext-align: left;\\n\\t\\t}\\n\\n\\t\\t.lt {\\n\\t\\t\\tcolor: inherit;\\n\\t\\t\\ttext-decoration: inherit;\\n\\t\\t}\\n\\n\\t\\t.l {\\n\\t\\t\\tcolor: #050;\\n\\t\\t}\\n\\n\\t\\t.s {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.c {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t.e {\\n\\t\\t\\t/*background-color: white; Changing background color */\\n\\t\\t\\tborder-radius: 10px;\\n\\t\\t\\t/* Making border radius */\\n\\t\\t\\twidth: 50%;\\n\\t\\t\\t/* Making auto-sizable width */\\n\\t\\t\\tpadding: 0 0 0 0;\\n\\t\\t\\t/* Making space around letters */\\n\\t\\t\\tfont-size: 14px;\\n\\t\\t\\t/* Changing font size */\\n\\t\\t\\tmargin-bottom: 0;\\n\\t\\t}\\n\\n\\t\\t.t {\\n\\t\\t\\tdisplay: inline-block;\\n\\t\\t}\\n\\n\\t\\t#player-div {\\n\\t\\t\\tposition: sticky;\\n\\t\\t\\ttop: 20px;\\n\\t\\t\\tfloat: right;\\n\\t\\t\\twidth: 40%\\n\\t\\t}\\n\\n\\t\\t#player {\\n\\t\\t\\taspect-ratio: 16 / 9;\\n\\t\\t\\twidth: 100%;\\n\\t\\t\\theight: auto;\\n\\t\\t}\\n\\n\\t\\ta {\\n\\t\\t\\tdisplay: inline;\\n\\t\\t}\\n\\t</style>';\n",
        "    preS += '\\n\\t<script>\\n\\twindow.onload = function () {\\n\\t\\t\\tvar player = document.getElementById(\"audio_player\");\\n\\t\\t\\tvar player;\\n\\t\\t\\tvar lastword = null;\\n\\n\\t\\t\\t// So we can compare against new updates.\\n\\t\\t\\tvar lastTimeUpdate = \"-1\";\\n\\n\\t\\t\\tsetInterval(function () {\\n\\t\\t\\t\\t// currentTime is checked very frequently (1 millisecond),\\n\\t\\t\\t\\t// but we only care about whole second changes.\\n\\t\\t\\t\\tvar ts = (player.currentTime).toFixed(1).toString();\\n\\t\\t\\t\\tts = (Math.round((player.currentTime) * 5) / 5).toFixed(1);\\n\\t\\t\\t\\tts = ts.toString();\\n\\t\\t\\t\\tconsole.log(ts);\\n\\t\\t\\t\\tif (ts !== lastTimeUpdate) {\\n\\t\\t\\t\\t\\tlastTimeUpdate = ts;\\n\\n\\t\\t\\t\\t\\t// Its now up to you to format the time.\\n\\t\\t\\t\\t\\tword = document.getElementById(ts)\\n\\t\\t\\t\\t\\tif (word) {\\n\\t\\t\\t\\t\\t\\tif (lastword) {\\n\\t\\t\\t\\t\\t\\t\\tlastword.style.fontWeight = \"normal\";\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\tlastword = word;\\n\\t\\t\\t\\t\\t\\t//word.style.textDecoration = \"underline\";\\n\\t\\t\\t\\t\\t\\tword.style.fontWeight = \"bold\";\\n\\n\\t\\t\\t\\t\\t\\tlet toggle = document.getElementById(\"autoscroll\");\\n\\t\\t\\t\\t\\t\\tif (toggle.checked) {\\n\\t\\t\\t\\t\\t\\t\\tlet position = word.offsetTop - 20;\\n\\t\\t\\t\\t\\t\\t\\twindow.scrollTo({\\n\\t\\t\\t\\t\\t\\t\\t\\ttop: position,\\n\\t\\t\\t\\t\\t\\t\\t\\tbehavior: \"smooth\"\\n\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}, 0.1);\\n\\t\\t}\\n\\n\\t\\tfunction jumptoTime(timepoint, id) {\\n\\t\\t\\tvar player = document.getElementById(\"audio_player\");\\n\\t\\t\\thistory.pushState(null, null, \"#\" + id);\\n\\t\\t\\tplayer.pause();\\n\\t\\t\\tplayer.currentTime = timepoint;\\n\\t\\t\\tplayer.play();\\n\\t\\t}\\n\\t\\t</script>\\n\\t</head>';\n",
        "    preS += '\\n\\n<body>\\n\\t<h2>' + audio_title + '</h2>\\n\\t<i>Click on a part of the transcription, to jump to its portion of audio, and get an anchor to it in the address\\n\\t\\tbar<br><br></i>\\n\\t<div id=\"player-div\">\\n\\t\\t<div id=\"player\">\\n\\t\\t\\t<audio controls=\"controls\" id=\"audio_player\">\\n\\t\\t\\t\\t<source src=\"input.wav\" />\\n\\t\\t\\t</audio>\\n\\t\\t</div>\\n\\t\\t<div><label for=\"autoscroll\">auto-scroll: </label>\\n\\t\\t\\t<input type=\"checkbox\" id=\"autoscroll\" checked>\\n\\t\\t</div>\\n\\t</div>\\n';\n",
        "\n",
        "postS = '\\t</body>\\n</html>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqO6Nd6YfZYa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#import webvtt\n",
        "import json\n",
        "from datetime import timedelta\n",
        "\n",
        "def timeStr(t):\n",
        "  return '{0:02d}:{1:02d}:{2:06.2f}'.format(round(t // 3600),\n",
        "                                                round(t % 3600 // 60),\n",
        "                                                t % 60)\n",
        "\n",
        "html = list(preS)\n",
        "txt = list(\"\")\n",
        "gidx = -1\n",
        "for g in groups:\n",
        "  shift = re.findall('[0-9]+:[0-9]+:[0-9]+\\.[0-9]+', string=g[0])[0]\n",
        "  shift = millisec(shift) - spacermilli #the start time in the original video\n",
        "  shift=max(shift, 0)\n",
        "\n",
        "  gidx += 1\n",
        "\n",
        "  captions = json.load(open(str(gidx) + '.json'))['segments']\n",
        "\n",
        "  if captions:\n",
        "    speaker = g[0].split()[-1]\n",
        "    boxclr = def_boxclr\n",
        "    spkrclr = def_spkrclr\n",
        "    if speaker in speakers:\n",
        "      speaker, boxclr, spkrclr = speakers[speaker]\n",
        "\n",
        "    html.append(f'<div class=\"e\" style=\"background-color: {boxclr}\">\\n');\n",
        "    html.append('<p  style=\"margin:0;padding: 5px 10px 10px 10px;word-wrap:normal;white-space:normal;\">\\n')\n",
        "    html.append(f'<span style=\"color:{spkrclr};font-weight: bold;\">{speaker}</span><br>\\n\\t\\t\\t\\t')\n",
        "\n",
        "    for c in captions:\n",
        "      start = shift + c['start'] * 1000.0\n",
        "      start = start / 1000.0   #time resolution ot youtube is Second.\n",
        "      end = (shift + c['end'] * 1000.0) / 1000.0\n",
        "      txt.append(f'[{timeStr(start)} --> {timeStr(end)}] [{speaker}] {c[\"text\"]}\\n')\n",
        "\n",
        "      for i, w in enumerate(c['words']):\n",
        "        if w == \"\":\n",
        "           continue\n",
        "        start = (shift + w['start']*1000.0) / 1000.0\n",
        "        #end = (shift + w['end']) / 1000.0   #time resolution ot youtube is Second.\n",
        "        html.append(f'<a href=\"#{timeStr(start)}\" id=\"{\"{:.1f}\".format(round(start*5)/5)}\" class=\"lt\" onclick=\"jumptoTime({int(start)}, this.id)\">{w[\"word\"]}</a><!--\\n\\t\\t\\t\\t-->')\n",
        "    #html.append('\\n')\n",
        "    html.append('</p>\\n')\n",
        "    html.append(f'</div>\\n')\n",
        "\n",
        "html.append(postS)\n",
        "\n",
        "\n",
        "with open(f\"capspeaker.txt\", \"w\", encoding='utf-8') as file:\n",
        "  s = \"\".join(txt)\n",
        "  file.write(s)\n",
        "  print('captions saved to capspeaker.txt:')\n",
        "  print(s+'\\n')\n",
        "\n",
        "with open(f\"capspeaker.html\", \"w\", encoding='utf-8') as file:    #TODO: proper html embed tag when video/audio from file\n",
        "  s = \"\".join(html)\n",
        "  file.write(s)\n",
        "  print('captions saved to capspeaker.html:')\n",
        "  print(s+'\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
